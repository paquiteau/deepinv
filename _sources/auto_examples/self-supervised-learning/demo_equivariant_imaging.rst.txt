
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-21

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 22-25

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 25-36

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 37-46

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 46-60

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      1%|▏         | 5.25M/399M [00:00<00:07, 52.5MiB/s]      3%|▎         | 12.4M/399M [00:00<00:06, 63.6MiB/s]      5%|▍         | 19.8M/399M [00:00<00:05, 68.5MiB/s]      7%|▋         | 27.2M/399M [00:00<00:05, 70.4MiB/s]      9%|▊         | 34.5M/399M [00:00<00:05, 71.4MiB/s]     11%|█         | 41.9M/399M [00:00<00:04, 72.3MiB/s]     12%|█▏        | 49.2M/399M [00:00<00:04, 72.7MiB/s]     14%|█▍        | 56.8M/399M [00:00<00:04, 73.6MiB/s]     16%|█▌        | 64.4M/399M [00:00<00:04, 74.4MiB/s]     18%|█▊        | 71.9M/399M [00:01<00:04, 74.7MiB/s]     20%|█▉        | 79.5M/399M [00:01<00:04, 74.9MiB/s]     22%|██▏       | 87.0M/399M [00:01<00:04, 74.9MiB/s]     24%|██▎       | 94.5M/399M [00:01<00:05, 53.3MiB/s]     26%|██▌       | 102M/399M [00:01<00:05, 58.4MiB/s]      27%|██▋       | 110M/399M [00:01<00:04, 62.8MiB/s]     29%|██▉       | 117M/399M [00:01<00:04, 66.1MiB/s]     31%|███       | 125M/399M [00:01<00:04, 68.4MiB/s]     33%|███▎      | 132M/399M [00:01<00:03, 70.3MiB/s]     35%|███▍      | 139M/399M [00:02<00:03, 71.1MiB/s]     37%|███▋      | 147M/399M [00:02<00:03, 72.3MiB/s]     39%|███▊      | 154M/399M [00:02<00:03, 72.7MiB/s]     41%|████      | 162M/399M [00:02<00:03, 71.8MiB/s]     42%|████▏     | 169M/399M [00:02<00:03, 70.3MiB/s]     44%|████▍     | 176M/399M [00:02<00:03, 71.3MiB/s]     46%|████▌     | 183M/399M [00:02<00:03, 71.1MiB/s]     48%|████▊     | 191M/399M [00:02<00:03, 66.9MiB/s]     50%|████▉     | 197M/399M [00:02<00:03, 59.6MiB/s]     51%|█████▏    | 205M/399M [00:03<00:03, 63.3MiB/s]     53%|█████▎    | 212M/399M [00:03<00:02, 66.0MiB/s]     55%|█████▌    | 219M/399M [00:03<00:02, 67.9MiB/s]     57%|█████▋    | 227M/399M [00:03<00:02, 69.3MiB/s]     59%|█████▊    | 234M/399M [00:03<00:02, 70.5MiB/s]     61%|██████    | 241M/399M [00:03<00:02, 71.4MiB/s]     62%|██████▏   | 249M/399M [00:03<00:02, 72.4MiB/s]     64%|██████▍   | 256M/399M [00:03<00:01, 72.9MiB/s]     66%|██████▌   | 264M/399M [00:03<00:01, 73.3MiB/s]     68%|██████▊   | 271M/399M [00:03<00:01, 68.2MiB/s]     70%|██████▉   | 278M/399M [00:04<00:01, 70.0MiB/s]     72%|███████▏  | 286M/399M [00:04<00:01, 71.5MiB/s]     74%|███████▎  | 293M/399M [00:04<00:01, 64.2MiB/s]     75%|███████▌  | 300M/399M [00:04<00:01, 61.0MiB/s]     77%|███████▋  | 307M/399M [00:04<00:01, 64.1MiB/s]     79%|███████▉  | 314M/399M [00:04<00:01, 66.8MiB/s]     81%|████████  | 322M/399M [00:04<00:01, 68.4MiB/s]     83%|████████▎ | 329M/399M [00:04<00:00, 69.7MiB/s]     84%|████████▍ | 336M/399M [00:04<00:00, 70.6MiB/s]     86%|████████▌ | 343M/399M [00:05<00:00, 71.6MiB/s]     88%|████████▊ | 351M/399M [00:05<00:00, 72.0MiB/s]     90%|████████▉ | 358M/399M [00:05<00:00, 72.5MiB/s]     92%|█████████▏| 365M/399M [00:05<00:00, 72.6MiB/s]     94%|█████████▎| 373M/399M [00:05<00:00, 72.6MiB/s]     95%|█████████▌| 380M/399M [00:05<00:00, 73.2MiB/s]     97%|█████████▋| 388M/399M [00:05<00:00, 73.3MiB/s]     99%|█████████▉| 395M/399M [00:05<00:00, 61.6MiB/s]    100%|██████████| 399M/399M [00:05<00:00, 68.4MiB/s]
    /home/runner/work/deepinv/deepinv/deepinv/utils/demo.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      x = torch.load(str(root_dir) + ".pt")




.. GENERATED FROM PYTHON SOURCE LINES 61-65

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 65-95

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 96-101

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 101-145

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 146-159

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 159-184

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 59.3MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 185-188

Train the network
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 188-220

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    # Initialize the trainer
    trainer = dinv.Trainer(
        model,
        physics=physics,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        optimizer=optimizer,
        train_dataloader=train_dataloader,
        plot_images=True,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.
        ckp_interval=10,
    )

    model = trainer.train()




.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: Ground truth, Measurement, Reconstruction
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
    Train epoch 0: MCLoss=0.0, EILoss=0.0, TotalLoss=0.0, PSNR=36.182




.. GENERATED FROM PYTHON SOURCE LINES 221-225

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 225-227

.. code-block:: Python


    trainer.test(test_dataloader)



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :alt: Ground truth, Measurement, No learning, Reconstruction
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
    Eval epoch 0: PSNR=35.264, PSNR no learning=29.389
    Test results:
    PSNR no learning: 29.389 +- 3.411
    PSNR: 35.264 +- 2.623

    {'PSNR no learning': 29.388799536718082, 'PSNR no learning_std': 3.4114185158882959, 'PSNR': 35.263515028235034, 'PSNR_std': 2.6233009603086459}




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 19.311 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_equivariant_imaging.zip <demo_equivariant_imaging.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
